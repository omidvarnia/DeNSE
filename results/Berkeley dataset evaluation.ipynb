{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Wavefront set extractor for Berkeley evaluation </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "notebook_path = Path(os.getcwd())\n",
    "parent_path = str(notebook_path.parent)\n",
    "dense_path = str(parent_path + '/dense')\n",
    "\n",
    "sys.path.append(dense_path)\n",
    "sys.path.append(str(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dense import shearlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import batch_gen as bg\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "with h5py.File(\"angle2berkeley.h5\", 'r') as h5:\n",
    "    X_train = h5[\"X_train\"][:]\n",
    "    X_test = h5[\"X_test\"][:]\n",
    "    X_valid = h5[\"X_valid\"][:]\n",
    "    y_train = h5[\"y_train\"][:]\n",
    "    y_test = h5[\"y_test\"][:]\n",
    "    y_valid = h5[\"y_valid\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_valid = y_valid.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_valid.shape, y_train.shape, y_test.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from adler.tensorflow import prelu, cosine_decay\n",
    "import os\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "#name = os.path.splitext(os.path.basename(__file__))[0]\n",
    "name = os.path.splitext(os.getcwd())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 2)\n",
    "y_test = to_categorical(y_test, num_classes = 2)\n",
    "y_valid = to_categorical(y_valid, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = X_train.shape[1]\n",
    "height = X_train.shape[2]\n",
    "channels = X_train.shape[3]\n",
    "nLabel = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weight Initialization\n",
    "# Create lots of weights and biases & Initialize with a small positive number as we will use ReLU\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "## Convolution and Pooling\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') \n",
    "\n",
    "## Pooling: max pooling over 2x2 blocks\n",
    "def max_pool_2x2(x): \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = 4\n",
    "fully_connected = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.float32, shape=(None, width, height,channels))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, nLabel))\n",
    "    \n",
    "    ## First Convolutional Layer\n",
    "    W_conv1 = weight_variable([3, 3, 49, 49*4])\n",
    "    b_conv1 = bias_variable([49*4])\n",
    "    #Convolution\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    print('h_conv1',h_conv1.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean1, batch_var1 = tf.nn.moments(h_conv1,[0])\n",
    "    h_conv1hat = (h_conv1-batch_mean1) / tf.sqrt(batch_var1 + 1e-3)\n",
    "    # Pooling\n",
    "    #h_pool1 = max_pool_2x2(h_conv1hat) \n",
    "    #print('h_pool1',h_pool1.shape)\n",
    "    # No_pooling\n",
    "    h_pool1 = h_conv1hat\n",
    "    print('h_pool1',h_pool1.shape)\n",
    "    \n",
    "    ## Second Convolutional Layer\n",
    "    W_conv2 = weight_variable([3, 3, 49*4, 49*4*4])\n",
    "    b_conv2 = bias_variable([49*4*4])\n",
    "    #Convolution\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    print('h_conv2',h_conv2.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean2, batch_var2 = tf.nn.moments(h_conv2,[0])\n",
    "    h_conv2hat = (h_conv2-batch_mean2) / tf.sqrt(batch_var2 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool2 = max_pool_2x2(h_conv2hat) \n",
    "    print('h_pool2',h_pool2.shape)\n",
    "    \n",
    "    ## Third Convolutional Layer\n",
    "    W_conv3 = weight_variable([3, 3, 49*4*4, 49*4*4*2])\n",
    "    b_conv3 = bias_variable([49*4*4*2])\n",
    "    #Convolution\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    print('h_conv3',h_conv3.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean3, batch_var3 = tf.nn.moments(h_conv3,[0])\n",
    "    h_conv3hat = (h_conv3-batch_mean3) / tf.sqrt(batch_var3 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool3 = max_pool_2x2(h_conv3hat) \n",
    "    print('h_pool3',h_pool3.shape)\n",
    "\n",
    "    \n",
    "    ## Third Convolutional Layer\n",
    "    W_conv4 = weight_variable([3, 3, 49*4*4*2, 49*4*4*2*2])\n",
    "    b_conv4 = bias_variable([49*4*4*2*2])\n",
    "    #Convolution\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "    print('h_conv4',h_conv4.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean4, batch_var4 = tf.nn.moments(h_conv4,[0])\n",
    "    h_conv4hat = (h_conv4-batch_mean4) / tf.sqrt(batch_var4 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool4 = max_pool_2x2(h_conv4hat) \n",
    "    print('h_pool4',h_pool4.shape)\n",
    "\n",
    "    ## Densely Connected Layer \n",
    "\n",
    "    # new shapes of pooled vectors\n",
    "    _, width_pooled, height_pooled, channels_pooled = h_pool4.shape\n",
    "\n",
    "    # fully-connected layer with 1024 neurons to process on the entire image\n",
    "    W_fc1 = weight_variable([int(width_pooled*height_pooled*channels_pooled), 1024])  \n",
    "    b_fc1 = bias_variable([1024])\n",
    "    \n",
    "    # Flat the output of the convolutional labels\n",
    "    h_pool4_flat = tf.reshape(h_pool4, [-1, int(width_pooled*height_pooled*channels_pooled)])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    print('h_pool4_flat',h_pool4_flat.shape)\n",
    "\n",
    "    ## Dropout (to reduce overfitting; useful when training very large neural network)\n",
    "    # We will turn on dropout during training & turn off during testing\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    ## Readout Layer\n",
    "\n",
    "    W_fc2 = weight_variable([1024, nLabel]) # [1024, 10]\n",
    "    b_fc2 = bias_variable([nLabel]) # [10]\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    predict = tf.nn.softmax(y_conv)\n",
    "\n",
    "    # set up for optimization (optimizer:ADAM)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)  # 1e-4\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    mf_score = tf.contrib.metrics.f1_score(tf.argmax(y_conv,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_minibatches = []\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_berkeley/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = accuracy.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, test accuracy %g\"%(step, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Computing f-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_berkeley/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test+100) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = mf_score.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, mf-score %g\"%(step, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = accuracy.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, test accuracy %g\"%(step, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the weights **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    plt.imshow(W_conv1[:,:,0,3].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = X_test[10:11,:,:,:]\n",
    "to_predict_label = y_test[10:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_berkeley/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    prediction2 = session.run(predict,feed_dict={x:to_predict,  keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
